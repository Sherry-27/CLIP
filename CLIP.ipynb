{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvOQWwrx_dg_",
        "outputId": "d5f83075-8a3d-4507-de33-a076b0a49185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "# CELL 1: Check GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: Install Dependencies\n",
        "print(\"ðŸ“¦ Installing dependencies...\")\n",
        "!pip install -q ftfy regex tqdm\n",
        "!pip install -q git+https://github.com/openai/CLIP.git\n",
        "!pip install -q faiss-gpu-cu12\n",
        "!pip install -q pillow\n",
        "!pip install -q matplotlib\n",
        "!pip install -q requests pandas\n",
        "print(\"âœ… All dependencies installed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UOgxoeVBDh-",
        "outputId": "e623ff83-aa34-4039-9070-a30ad5af3289"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¦ Installing dependencies...\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "âœ… All dependencies installed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Import Libraries\n",
        "import torch\n",
        "import clip\n",
        "import faiss\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "print(\"âœ… Libraries imported\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWTQsegQBgDz",
        "outputId": "e2320dce-b772-4bb3-c165-4eb6b9d22640"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Libraries imported\n",
            "PyTorch version: 2.9.0+cpu\n",
            "CUDA available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Load CLIP Model\n",
        "print(\"\\nðŸš€ Loading CLIP model...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "print(f\"âœ… CLIP model loaded on {device}\")\n",
        "print(f\"Model: ViT-B/32\")\n",
        "print(f\"Input resolution: 224x224\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M84tt-I1BvZS",
        "outputId": "a9428e88-71d7-4b09-e04f-b62a8de5d5c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ Loading CLIP model...\n",
            "âœ… CLIP model loaded on cpu\n",
            "Model: ViT-B/32\n",
            "Input resolution: 224x224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Download Multiple Small Datasets\n",
        "\n",
        "print(\"\\nðŸ“¥ Downloading multiple image datasets...\")\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# â”€â”€ 1. COCO val2017 â”€â”€ (~5,000 images)\n",
        "print(\"â†’ Downloading & extracting COCO val2017...\")\n",
        "os.makedirs(\"images/coco\", exist_ok=True)\n",
        "\n",
        "# Download if not already present\n",
        "coco_zip = \"val2017.zip\"\n",
        "if not os.path.exists(coco_zip):\n",
        "    !wget -q http://images.cocodataset.org/zips/val2017.zip -O {coco_zip}\n",
        "\n",
        "# Extract with overwrite (non-interactive)\n",
        "!unzip -q -o {coco_zip} -d images/coco > /dev/null 2>&1\n",
        "\n",
        "# Clean up zip\n",
        "if os.path.exists(coco_zip):\n",
        "    os.remove(coco_zip)\n",
        "\n",
        "coco_count = len(list(Path(\"images/coco/val2017\").glob(\"*.jpg\")))\n",
        "print(f\"   â†’ COCO images extracted: {coco_count}\")\n",
        "\n",
        "# â”€â”€ 2. Flickr30k images â”€â”€ (reliable public mirror)\n",
        "print(\"â†’ Downloading Flickr30k images (from awsaf49 GitHub mirror)...\")\n",
        "os.makedirs(\"images/flickr30k\", exist_ok=True)\n",
        "\n",
        "# Download the three split parts\n",
        "parts = [\n",
        "    \"flickr30k_part00\",\n",
        "    \"flickr30k_part01\",\n",
        "    \"flickr30k_part02\"\n",
        "]\n",
        "\n",
        "for part in parts:\n",
        "    url = f\"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/{part}\"\n",
        "    !wget -q {url} -O {part}\n",
        "\n",
        "# Combine into one zip\n",
        "!cat {' '.join(parts)} > flickr30k.zip 2>/dev/null\n",
        "\n",
        "# Clean up parts\n",
        "for part in parts:\n",
        "    if os.path.exists(part):\n",
        "        os.remove(part)\n",
        "\n",
        "# Extract non-interactively\n",
        "print(\"   â†’ Extracting Flickr30k...\")\n",
        "!unzip -q -o flickr30k.zip -d flickr_temp > /dev/null 2>&1\n",
        "\n",
        "# Move images (some mirrors have different folder structure)\n",
        "!mkdir -p images/flickr30k\n",
        "!find flickr_temp -type f -name \"*.jpg\" -exec mv {} images/flickr30k/ \\; 2>/dev/null || true\n",
        "\n",
        "# Clean up temp files\n",
        "!rm -rf flickr_temp flickr30k.zip\n",
        "\n",
        "# Limit to first 9000 images (adjust as needed)\n",
        "flickr_paths = sorted(Path(\"images/flickr30k\").glob(\"*.jpg\"))\n",
        "flickr_selected = flickr_paths[:9000]\n",
        "flickr_count = len(flickr_selected)\n",
        "print(f\"   â†’ Using {flickr_count} images from Flickr30k\")\n",
        "\n",
        "# â”€â”€ Create unified image directory â”€â”€\n",
        "print(\"â†’ Creating unified image folder...\")\n",
        "os.makedirs(\"images/all\", exist_ok=True)\n",
        "\n",
        "# Copy COCO images (non-overwrite)\n",
        "!cp -n images/coco/val2017/*.jpg images/all/ 2>/dev/null || true\n",
        "\n",
        "# Copy selected Flickr30k images\n",
        "for src in flickr_selected:\n",
        "    !cp -n \"{src}\" images/all/ 2>/dev/null || true\n",
        "\n",
        "# Final count\n",
        "total_images = len(list(Path(\"images/all\").glob(\"*.jpg\")))\n",
        "print(f\"Total images ready for encoding: {total_images}\")\n",
        "print(f\"   â†’ Directory: images/all\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InotxiPbCO-g",
        "outputId": "11d8f00c-6df9-414b-db1c-7d3820535b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“¥ Downloading multiple image datasets...\n",
            "â†’ Downloading & extracting COCO val2017...\n",
            "   â†’ COCO images extracted: 5000\n",
            "â†’ Downloading Flickr30k images (from awsaf49 GitHub mirror)...\n",
            "   â†’ Extracting Flickr30k...\n",
            "   â†’ Using 9000 images from Flickr30k\n",
            "â†’ Creating unified image folder...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Assuming IMAGE_DIR is still 'images/all'\n",
        "base_dir = os.path.abspath(IMAGE_DIR)  # e.g. /content/images/all\n",
        "\n",
        "# Fix all paths to be relative or absolute based on current location\n",
        "fixed_paths = []\n",
        "missing_count = 0\n",
        "\n",
        "print(\"Fixing image paths...\")\n",
        "for old_path in search_engine.image_paths:\n",
        "    filename = os.path.basename(old_path)\n",
        "    new_path = os.path.join(base_dir, filename)\n",
        "\n",
        "    if os.path.exists(new_path):\n",
        "        fixed_paths.append(new_path)\n",
        "    else:\n",
        "        fixed_paths.append(None)  # or skip, but keep index aligned\n",
        "        missing_count += 1\n",
        "        print(f\"Still missing: {filename}\")\n",
        "\n",
        "search_engine.image_paths = fixed_paths\n",
        "\n",
        "print(f\"Path fix complete.\")\n",
        "print(f\"Total images in index: {len(search_engine.image_paths)}\")\n",
        "print(f\"Still missing files: {missing_count}\")\n",
        "print(f\"Example fixed path: {search_engine.image_paths[0] if search_engine.image_paths else 'None'}\")"
      ],
      "metadata": {
        "id": "wREvR5ZhTd0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: CLIPImageSearch Class (same as before)\n",
        "\n",
        "class CLIPImageSearch:\n",
        "    def __init__(self, model, preprocess, device):\n",
        "        self.model = model\n",
        "        self.preprocess = preprocess\n",
        "        self.device = device\n",
        "        self.image_paths = []\n",
        "        self.image_features = None\n",
        "        self.index = None\n",
        "        self.dimension = 512\n",
        "\n",
        "    def encode_images(self, image_dir: str, batch_size: int = 32):\n",
        "        image_extensions = {'.jpg', '.jpeg', '.png'}\n",
        "        image_files = []\n",
        "        for ext in image_extensions:\n",
        "            image_files.extend(Path(image_dir).glob(f'*{ext}'))\n",
        "\n",
        "        self.image_paths = [str(p) for p in sorted(image_files)]\n",
        "        print(f\"ðŸ“ Found {len(self.image_paths)} images\")\n",
        "\n",
        "        if len(self.image_paths) == 0:\n",
        "            print(\"âŒ No images found!\")\n",
        "            return\n",
        "\n",
        "        all_features = []\n",
        "        for i in tqdm(range(0, len(self.image_paths), batch_size), desc=\"Encoding images\"):\n",
        "            batch_paths = self.image_paths[i:i+batch_size]\n",
        "            batch_images = []\n",
        "            for img_path in batch_paths:\n",
        "                try:\n",
        "                    image = Image.open(img_path).convert('RGB')\n",
        "                    image = self.preprocess(image)\n",
        "                    batch_images.append(image)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if not batch_images:\n",
        "                continue\n",
        "\n",
        "            batch_tensor = torch.stack(batch_images).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                features = self.model.encode_image(batch_tensor)\n",
        "                features = features / features.norm(dim=-1, keepdim=True)\n",
        "            all_features.append(features.cpu().numpy())\n",
        "\n",
        "        if all_features:\n",
        "            self.image_features = np.vstack(all_features)\n",
        "            print(f\"âœ… Encoded {len(self.image_features)} images\")\n",
        "        else:\n",
        "            print(\"âŒ Encoding failed\")\n",
        "\n",
        "    def build_index(self, index_type='flatip'):\n",
        "        if self.image_features is None:\n",
        "            print(\"âŒ No features! Run encode_images first.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Building FAISS index ({index_type})...\")\n",
        "\n",
        "        if index_type == 'flatl2':\n",
        "            self.index = faiss.IndexFlatL2(self.dimension)\n",
        "        elif index_type == 'flatip':\n",
        "            self.index = faiss.IndexFlatIP(self.dimension)\n",
        "        elif index_type == 'ivf':\n",
        "            nlist = 100\n",
        "            quantizer = faiss.IndexFlatL2(self.dimension)\n",
        "            self.index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)\n",
        "            self.index.train(self.image_features)\n",
        "\n",
        "        self.index.add(self.image_features.astype('float32'))\n",
        "        print(f\"âœ… Index built â€” {self.index.ntotal} vectors\")\n",
        "\n",
        "    def search_by_text(self, query: str, top_k: int = 10) -> List[Tuple[str, float]]:\n",
        "        text_tokens = clip.tokenize([query]).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            text_features = self.model.encode_text(text_tokens)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features.cpu().numpy().astype('float32')\n",
        "\n",
        "        distances, indices = self.index.search(text_features, top_k)\n",
        "\n",
        "        results = []\n",
        "        for idx, dist in zip(indices[0], distances[0]):\n",
        "            if idx < len(self.image_paths):\n",
        "                results.append((self.image_paths[idx], float(dist)))\n",
        "        return results\n",
        "\n",
        "    def save_index(self, save_dir: str):\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        faiss.write_index(self.index, os.path.join(save_dir, 'image_index.faiss'))\n",
        "        metadata = {\n",
        "            'image_paths': self.image_paths,\n",
        "            'num_images': len(self.image_paths),\n",
        "            'dimension': self.dimension\n",
        "        }\n",
        "        with open(os.path.join(save_dir, 'metadata.json'), 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        np.save(os.path.join(save_dir, 'image_features.npy'), self.image_features)\n",
        "        print(f\"Index saved â†’ {save_dir}\")\n",
        "\n",
        "print(\"âœ… CLIPImageSearch class ready\")"
      ],
      "metadata": {
        "id": "WFYltaeGCSW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Build & Save Index\n",
        "\n",
        "search_engine = CLIPImageSearch(model, preprocess, device)\n",
        "\n",
        "print(\"\\nEncoding images (this will take several minutes)...\")\n",
        "search_engine.encode_images(IMAGE_DIR, batch_size=48)   # adjust batch size according to GPU memory\n",
        "\n",
        "print(\"\\nBuilding index...\")\n",
        "search_engine.build_index(index_type='flatip')          # cosine similarity\n",
        "\n",
        "print(\"\\nSaving index...\")\n",
        "search_engine.save_index('clip_index_small')"
      ],
      "metadata": {
        "id": "z3l4XJivRtu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: Quick Test â€“ Text Search\n",
        "\n",
        "test_queries = [\n",
        "    \"a cute dog running in the park\",\n",
        "    \"person riding a bicycle\",\n",
        "    \"beautiful mountain landscape\",\n",
        "    \"plate of delicious food\",\n",
        "    \"woman wearing red dress\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    results = search_engine.search_by_text(query, top_k=5)\n",
        "    for i, (path, score) in enumerate(results, 1):\n",
        "        print(f\"  {i}. {os.path.basename(path)}  â†’ score {score:.4f}\")"
      ],
      "metadata": {
        "id": "2L-_br94Rwxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_top_results(query, results, ncols=5):\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    plt.figure(figsize=(15, 4))\n",
        "    displayed = 0\n",
        "    for fname, score in results:\n",
        "        # Use basename only, combine with current IMAGE_DIR\n",
        "        img_path = os.path.join(IMAGE_DIR, os.path.basename(fname))\n",
        "        if os.path.exists(img_path):\n",
        "            img = Image.open(img_path)\n",
        "            plt.subplot(1, ncols, displayed + 1)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"{score:.4f}\")\n",
        "            plt.axis('off')\n",
        "            displayed += 1\n",
        "        else:\n",
        "            print(f\"Missing image: {os.path.basename(fname)}\")\n",
        "    if displayed == 0:\n",
        "        print(\"No images found for display\")\n",
        "    plt.suptitle(query, fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Test again\n",
        "for q in [\n",
        "    \"a cute dog running in the park\",\n",
        "    \"person riding a bicycle\",\n",
        "    \"plate of delicious food\"\n",
        "]:\n",
        "    results = search_engine.search_by_text(q, top_k=5)\n",
        "    show_top_results(q, results)"
      ],
      "metadata": {
        "id": "2xFFf-21TNkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save (or re-save) the index\n",
        "search_engine.save_index(\"clip_search_index\")\n",
        "\n",
        "# Optional: create a folder structure that's easier to upload\n",
        "!mkdir -p hf_space_files\n",
        "!cp -r clip_search_index hf_space_files/\n",
        "!cp -r images/all hf_space_files/images   # or only a subset if too big"
      ],
      "metadata": {
        "id": "spRIw7yTUIRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Load existing metadata\n",
        "with open(\"clip_search_index/metadata.json\", \"r\") as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "# Convert to filenames only\n",
        "metadata[\"image_paths\"] = [os.path.basename(p) for p in metadata[\"image_paths\"]]\n",
        "\n",
        "# Optional: also save num_images\n",
        "metadata[\"num_images\"] = len(metadata[\"image_paths\"])\n",
        "\n",
        "# Save updated version\n",
        "with open(\"clip_search_index/metadata_clean.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"Saved metadata with filenames only\")\n",
        "print(f\"Number of images referenced: {metadata['num_images']}\")"
      ],
      "metadata": {
        "id": "XLalwLGKVEPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import shutil\n",
        "\n",
        "subset_dir = \"hf_space_files/images_subset\"\n",
        "os.makedirs(subset_dir, exist_ok=True)\n",
        "\n",
        "all_images = list(Path(\"images/all\").glob(\"*.jpg\"))\n",
        "selected = random.sample(all_images, min(500, len(all_images)))\n",
        "\n",
        "for img_path in selected:\n",
        "    shutil.copy(img_path, subset_dir)\n",
        "\n",
        "print(f\"Copied {len(selected)} images to images_subset/\")"
      ],
      "metadata": {
        "id": "4FQWTXp5WJIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U huggingface_hub\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Run this cell and follow the link to authenticate\n",
        "login()   # â† paste your Hugging Face access token here"
      ],
      "metadata": {
        "id": "5eCf799XWWgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# ==============================================\n",
        "# CONFIG - change these if needed\n",
        "# ==============================================\n",
        "SPACE_ID = \"Shaheerkhan/clip-faiss-image-search\"   # your space\n",
        "LOCAL_IMAGE_FOLDER = \"images/all\"                   # where your full images are\n",
        "NUMBER_OF_IMAGES_TO_UPLOAD = 300                    # change this (100â€“800 is reasonable)\n",
        "PATH_IN_SPACE = \"images\"                            # folder name in the space\n",
        "\n",
        "# ==============================================\n",
        "# Prepare temporary folder for upload\n",
        "# ==============================================\n",
        "temp_upload_dir = \"temp_images_upload\"\n",
        "os.makedirs(temp_upload_dir, exist_ok=True)\n",
        "\n",
        "# Get all images\n",
        "all_images = list(Path(LOCAL_IMAGE_FOLDER).glob(\"*.jpg\"))\n",
        "\n",
        "if not all_images:\n",
        "    print(\"Error: No images found in\", LOCAL_IMAGE_FOLDER)\n",
        "else:\n",
        "    print(f\"Found {len(all_images)} images total\")\n",
        "\n",
        "    # Select random subset\n",
        "    selected_images = random.sample(all_images, min(NUMBER_OF_IMAGES_TO_UPLOAD, len(all_images)))\n",
        "    print(f\"Selected {len(selected_images)} images to upload\")\n",
        "\n",
        "    # Copy to temp folder\n",
        "    for img_path in selected_images:\n",
        "        shutil.copy(img_path, temp_upload_dir)\n",
        "\n",
        "    print(\"Temporary folder prepared:\", temp_upload_dir)\n",
        "\n",
        "    # Show first few files (fixed version)\n",
        "    files = list(Path(temp_upload_dir).glob(\"*\"))\n",
        "    if files:\n",
        "        print(\"First few files:\", [f.name for f in files[:5]])\n",
        "    else:\n",
        "        print(\"No files were copied â€“ check if source images exist\")\n",
        "\n",
        "    # ==============================================\n",
        "    # Upload to Hugging Face Space\n",
        "    # ==============================================\n",
        "    api = HfApi()\n",
        "\n",
        "    print(f\"\\nUploading {len(selected_images)} images to {SPACE_ID}/{PATH_IN_SPACE}/ ...\")\n",
        "\n",
        "    api.upload_folder(\n",
        "        folder_path=temp_upload_dir,\n",
        "        repo_id=SPACE_ID,\n",
        "        repo_type=\"space\",\n",
        "        path_in_repo=PATH_IN_SPACE,           # creates images/ folder if needed\n",
        "        commit_message=f\"Add {len(selected_images)} demo images\",\n",
        "        ignore_patterns=[\"*.tmp\", \"__pycache__\"]\n",
        "    )\n",
        "\n",
        "    print(\"\\nUpload finished!\")\n",
        "    print(f\"Images should now appear in: https://huggingface.co/spaces/{SPACE_ID}/tree/main/{PATH_IN_SPACE}\")"
      ],
      "metadata": {
        "id": "XGyNLKu-X8nA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Point to your local images folder\n",
        "image_folder = \"images/all\"   # or wherever your full set is\n",
        "\n",
        "# Get all .jpg filenames\n",
        "image_files = [f.name for f in Path(image_folder).glob(\"*.jpg\")]\n",
        "\n",
        "# Create metadata structure (filenames only)\n",
        "metadata = {\n",
        "    \"image_paths\": image_files,          # just names like \"000000123456.jpg\"\n",
        "    \"num_images\": len(image_files),\n",
        "    \"dimension\": 512                     # CLIP ViT-B/32 dimension\n",
        "}\n",
        "\n",
        "# Save it locally\n",
        "os.makedirs(\"clip_search_index\", exist_ok=True)\n",
        "with open(\"clip_search_index/metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\"Created metadata.json with {len(image_files)} filenames\")\n",
        "print(\"First few filenames:\", image_files[:5])"
      ],
      "metadata": {
        "id": "ZpB_MF6jYsfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()   # â† just call it like this"
      ],
      "metadata": {
        "id": "RHb1E26fdwes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"clip_search_index/metadata.json\",\n",
        "    path_in_repo=\"clip_search_index/metadata.json\",\n",
        "    repo_id=\"Shaheerkhan/clip-faiss-image-search\",\n",
        "    repo_type=\"space\",\n",
        "    commit_message=\"Add missing metadata.json\"\n",
        ")"
      ],
      "metadata": {
        "id": "6WQ3brL7cs5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api.upload_folder(\n",
        "    folder_path=\"clip_search_index\",               # folder with metadata.json + image_index.faiss\n",
        "    repo_id=\"Shaheerkhan/clip-faiss-image-search\",\n",
        "    repo_type=\"space\",\n",
        "    path_in_repo=\"clip_search_index\",\n",
        "    commit_message=\"Add FAISS index and metadata\"\n",
        ")"
      ],
      "metadata": {
        "id": "CLF6AmNcc16L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-run these lines (adjust if your variable name is different)\n",
        "search_engine.build_index(index_type='flatip')\n",
        "search_engine.save_index('clip_search_index')"
      ],
      "metadata": {
        "id": "uQZCRNYwdAVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh clip_search_index/"
      ],
      "metadata": {
        "id": "ggnCABUsinbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# Upload FAISS index\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"clip_search_index/image_index.faiss\",\n",
        "    path_in_repo=\"clip_search_index/image_index.faiss\",\n",
        "    repo_id=\"Shaheerkhan/clip-faiss-image-search\",\n",
        "    repo_type=\"space\",\n",
        "    commit_message=\"Add missing image_index.faiss\"\n",
        ")\n",
        "\n",
        "# Re-upload metadata.json (to make sure it's fresh)\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"clip_search_index/metadata.json\",\n",
        "    path_in_repo=\"clip_search_index/metadata.json\",\n",
        "    repo_id=\"Shaheerkhan/clip-faiss-image-search\",\n",
        "    repo_type=\"space\",\n",
        "    commit_message=\"Update metadata.json\"\n",
        ")\n",
        "\n",
        "print(\"Upload complete! Wait 2â€“5 minutes then refresh Space.\")"
      ],
      "metadata": {
        "id": "84zP-zezj9z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh clip_search_index/\n",
        "!du -sh clip_search_index/image_index.faiss 2>/dev/null || echo \"image_index.faiss not found!\""
      ],
      "metadata": {
        "id": "qrkBfgMUkBmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import faiss\n",
        "import os\n",
        "import json\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "class CLIPImageSearch:\n",
        "    def __init__(self, model, preprocess, device):\n",
        "        self.model = model\n",
        "        self.preprocess = preprocess\n",
        "        self.device = device\n",
        "        self.image_paths = []\n",
        "        self.image_features = None\n",
        "        self.index = None\n",
        "        self.dimension = 512\n",
        "\n",
        "    def encode_images(self, image_dir, batch_size=16):\n",
        "        files = list(Path(image_dir).glob(\"*.jpg\"))\n",
        "        if not files:\n",
        "            print(\"No images found in\", image_dir)\n",
        "            return\n",
        "\n",
        "        self.image_paths = [str(p) for p in files]\n",
        "        print(f\"Found {len(self.image_paths)} images\")\n",
        "\n",
        "        features_list = []\n",
        "        for start in tqdm(range(0, len(files), batch_size)):\n",
        "            batch_paths = files[start:start+batch_size]\n",
        "            batch_imgs = []\n",
        "            for p in batch_paths:\n",
        "                try:\n",
        "                    img = Image.open(p).convert(\"RGB\")\n",
        "                    batch_imgs.append(self.preprocess(img))\n",
        "                except Exception as e:\n",
        "                    print(f\"Skip {p}: {e}\")\n",
        "            if not batch_imgs:\n",
        "                continue\n",
        "            tensor = torch.stack(batch_imgs).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                emb = self.model.encode_image(tensor)\n",
        "                emb = emb / emb.norm(dim=-1, keepdim=True)\n",
        "            features_list.append(emb.cpu().numpy())\n",
        "\n",
        "        if features_list:\n",
        "            self.image_features = np.concatenate(features_list)\n",
        "            print(f\"Encoded {len(self.image_features)} features successfully\")\n",
        "        else:\n",
        "            print(\"No features encoded - check image loading\")\n",
        "\n",
        "    def build_index(self):\n",
        "        if self.image_features is None or len(self.image_features) == 0:\n",
        "            print(\"No features to index!\")\n",
        "            return\n",
        "        self.index = faiss.IndexFlatIP(self.dimension)\n",
        "        self.index.add(self.image_features.astype('float32'))\n",
        "        print(f\"Index built with {self.index.ntotal} vectors\")\n",
        "\n",
        "    def save_index(self, save_dir):\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        if self.index is not None:\n",
        "            faiss.write_index(self.index, os.path.join(save_dir, \"image_index.faiss\"))\n",
        "        metadata = {\n",
        "            \"image_paths\": self.image_paths,\n",
        "            \"num_images\": len(self.image_paths),\n",
        "            \"dimension\": self.dimension\n",
        "        }\n",
        "        with open(os.path.join(save_dir, \"metadata.json\"), \"w\") as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        print(f\"Saved to {save_dir}\")\n",
        "        !ls -lh {save_dir}\n",
        "\n",
        "print(\"Class ready\")"
      ],
      "metadata": {
        "id": "m8i73NxAkpnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_engine = CLIPImageSearch(model, preprocess, device)\n",
        "\n",
        "# Use the subset you uploaded or full folder\n",
        "# Change path if needed\n",
        "search_engine.encode_images(\"temp_images_upload\", batch_size=16)  # or \"images/all\""
      ],
      "metadata": {
        "id": "s4yCxtdok7-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "possible_dirs = [\n",
        "    \"images/all\",\n",
        "    \"images\",\n",
        "    \"temp_images_upload\",\n",
        "    \"hf_space_files/images_subset\",\n",
        "    \"Images\",\n",
        "    \"images/coco/val2017\",\n",
        "    \"images/flickr30k\",\n",
        "    \"images/unsplash\"\n",
        "]\n",
        "\n",
        "for d in possible_dirs:\n",
        "    if Path(d).exists():\n",
        "        jpg_count = len(list(Path(d).glob(\"*.jpg\")))\n",
        "        print(f\"Folder '{d}': {jpg_count} .jpg files\")\n",
        "    else:\n",
        "        print(f\"Folder '{d}': does NOT exist\")"
      ],
      "metadata": {
        "id": "1PeEL_GvlBvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"clip_search_index/image_index.faiss\",\n",
        "    path_in_repo=\"clip_search_index/image_index.faiss\",\n",
        "    repo_id=\"Shaheerkhan/clip-faiss-image-search\",\n",
        "    repo_type=\"space\",\n",
        "    commit_message=\"Upload missing image_index.faiss\"\n",
        ")"
      ],
      "metadata": {
        "id": "KH721SjwlToA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VwqaNlH9l806"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}